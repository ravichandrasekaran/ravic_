---
title: 'Tidy: Potential fraud in Guess My Word '
author: ravic_
date: '2020-01-21'
slug: tidy-potential-fraud-in-guess-my-word
categories: []
draft: true
tags:
  - tidy-tuesday
---

## Motivation
Like others, I've been surprised by my own addiction to Ryan Jones's revival 
of "Guess My Word". 


## Data
I grabbed a [snapshot](ttps://docs.google.com/spreadsheets/d/1-LWgLc0AsWJhOJKoE9k8X52dUB_Rcd8mRHpQJjkbxRk/edit?usp=sharing) 
of the daily leaderboards from early 2020-01-21. For each participant, the 
number of guesses and the time to completion are recorded. In addition, entries
are annotated with certain completion notes - first to respond, fewest guesses,
fastest times, and "lucky". 







```{r load_packages, echo = FALSE}
options(tidyverse.quiet = TRUE)
require(tidyverse, quietly = TRUE, warn.conflicts = FALSE)
```


```{r import_data, echo = FALSE}
options(gargle_oauth_email = TRUE)
require(googledrive, quietly = TRUE, warn.conflicts = FALSE)
drive_auth(email = "ravichandrasekaran@gmail.com")

require("glue", quietly = TRUE, warn.conflicts = FALSE)
require("googlesheets4", quietly = TRUE, warn.conflicts = FALSE)
require("lubridate", quietly = TRUE, warn.conflicts = FALSE)



url <- glue::glue(
  "https://docs.google.com/spreadsheets/d/",
  "1-LWgLc0AsWJhOJKoE9k8X52dUB_Rcd8mRHpQJjkbxRk/edit?usp=sharing")
sheet_id <- sheets_get(url)$spreadsheet_id
df_normal <- read_sheet(sheet_id, sheet = "2020-01-21-normal") 
df_hard <- read_sheet(sheet_id, sheet = "2020-01-21-hard") 

```

```{r clean_data, echo = FALSE}
require("janitor", quietly = TRUE, warn.conflicts = FALSE)

clean_fields <- function(df) {
  df %>% 
    janitor::clean_names() %>%
    # Lubridate period will interpret lowercase 'm' as month, not minute
    mutate(
      time = str_replace(time, "m", "M"),
      time = lubridate::period(time),
      fraud_ind = if_else(is.na(awards), FALSE, str_detect(awards, "lucky")))
}

df <- bind_rows(normal = df_normal, hard = df_hard, .id = "difficulty") %>%
  mutate(difficulty = fct_relevel(difficulty, c("normal", "hard")))
df <- clean_fields(df)
```

```{r feature_eng, echo = FALSE}
df <- df %>%
  mutate(
    secs = as.numeric(seconds(time)),
    log_secs = log(secs, base = 10),
    guess_rate = secs / number_guesses)

```

```{r num_guess_plots, echo = FALSE}
require("patchwork", quietly = TRUE, warn.conflicts = FALSE)

df %>%
  count(difficulty, number_guesses) %>%
  ggplot(aes(number_guesses, n)) +
  geom_col(fill = "lightblue", position = "identity") +
  theme_light() +
  labs(
    title = "Distribution of number of guesses for leaderboard",
    x = "", y = ""
  ) +
  coord_cartesian(xlim = c(0, 50)) +
  facet_wrap(~ difficulty, ncol = 2)

hists <- df %>%
  count(difficulty, number_guesses) %>%
  ggplot(aes(number_guesses, n)) +
  geom_col(aes(fill = difficulty), 
           position = "identity", alpha = .3, show.legend = FALSE) +
  theme_light() +
  labs(
    title = "Number of guesses - histogram",
    x = "", y = ""
  ) +
  coord_cartesian(xlim = c(0, 50)) 

# https://stackoverflow.com/questions/36008865/creating-density-from-frequency-table
densities <- df %>%
  group_by(difficulty, number_guesses) %>%
  summarise(n = n()) %>%
  mutate(pct = n / sum(n)) %>%
  ungroup() %>%
  ggplot(aes(x = number_guesses, weight = pct)) +
  geom_density(aes(fill = difficulty, color = difficulty), alpha = .2) +
  theme_light() +
  labs(
    title = "Number of guesses - density",
    x = "", y = ""
  ) +
  coord_cartesian(xlim = c(0, 50))  

(hists | densities) +
  plot_layout(guides = "collect")

```


```{r fraud_by_guesses, echo = FALSE}
num_guess_summary <- df %>%
  group_by(difficulty) %>%
  summarise(
    n = n(), 
    mean_guesses = mean(number_guesses),
    sd_guesses = sd(number_guesses)
  ) %>%
  mutate(threshold = qnorm(.005, mean = mean_guesses, sd = sd_guesses))

num_guess_summary

df %>%
  left_join(num_guess_summary, by = "difficulty") %>%
  filter(number_guesses < threshold | fraud_ind) %>%
  arrange(difficulty, number_guesses) %>%
  select(difficulty, name, number_guesses, time, fraud_ind, threshold)
```

```{r fraud_by_time, echo = FALSE}
df %>%
  mutate(secs = as.numeric(seconds(time))) %>%
  filter(secs < quantile(df$secs, 0.975)) %>%
  ggplot(aes(secs)) +
  geom_histogram(bins = 30, fill = "lightblue") +
  scale_x_log10() +
  theme_light() +
  labs(
    title = "Time - histogram",
    subtitle = "Data limited to times below 1200s",
    x = "Time to completion (sec)", y = "") +
  facet_wrap(~ difficulty)

time_summary <- df %>%
  group_by(difficulty) %>%
  summarise(
    n = n(), 
    mean_log_secs = mean(log_secs),
    sd_log_secs = sd(log_secs)
  ) %>%
  mutate(threshold = 10 ^ qnorm(.001, mean = mean_log_secs, sd = sd_log_secs))

time_summary


df %>%
  left_join(time_summary, by = "difficulty") %>%
  filter(secs < threshold | fraud_ind) %>%
  arrange(difficulty, number_guesses) %>%
  select(difficulty, name, number_guesses, time, fraud_ind, awards, threshold)
```


### Guess rate
I'm defining *guess rate* as the number of guesses divided by the total 
completion time in seconds. With the guess rate, we can try to see if 
there is an unexpected barrage of overly quick guesses. 

Here's the distribution. (Notably, the guess rate is pretty similar 
between the normal and hard contests.) The guess rate frequency 
distribution looks log-normal, and the x-axis is on a log scale 
accordingly.

```{r guess_rate, echo = FALSE}

df %>% 
  ggplot(aes(guess_rate)) + 
  geom_histogram(bins = 30, fill = "lightblue") +
  scale_x_log10() +
  theme_light() +
  labs(
    title = "Guess rate distribution",
    subtitle = "Guess rate: number of guesses / elapsed sec",
    x = "Guesses per sec", y = "") +
  facet_wrap(~ difficulty)
```

Here's a quick look at the mean, standard deviation of the log-normal
estimated parameters. 
```{r guess_rate_summary, echo = FALSE}
guess_rate_summary <- df %>%
  group_by(difficulty) %>%
  summarise(
    n = n(), 
    mean_log_guess_rate = mean(log(guess_rate, base = 10)),
    sd_log_guess_rate = sd(log(guess_rate, base = 10))) %>%
  mutate(threshold = 10 ^ qnorm(.001, mean = mean_log_guess_rate, 
                                sd = sd_log_guess_rate))

guess_rate_summary
```


Also, have added in a 0.1% percentile threshold for identifying "ðŸ€ *lucky?*" 
users. **TK:** Might be better to assign the *p*-value for seeing an 
observation that extreme.

```{r guess_rate_id, echo = FALSE}
df %>%
  left_join(guess_rate_summary, by = "difficulty") %>%
  filter(guess_rate < threshold | fraud_ind) %>%
  arrange(difficulty, guess_rate) %>%
  select(difficulty, name, number_guesses, secs, guess_rate, 
         threshold, awards)

```

Guess rate is presumably a function of the number of guesses performed --
where fatigue and frustration set in. Let's find out.

```{r guess_rate_by_guesses, echo = FALSE}


df %>%
  filter(guess_rate < 50) %>%
  ggplot(aes(number_guesses, guess_rate)) + 
  geom_point() +
  facet_wrap(~ difficulty) +
  theme_light() +
  labs(
    title = "Guess rate vs. number of guesses",
    x = "Number of guesses", 
    y = "Guess rate (guesses per sec)")
```



```{r, echo = FALSE, eval = FALSE}
# Unfortunately, couldn't use simple rvest, since this is client side rendered.
# Would need to enable with a headless Chrome pipeline at the end of the day.

require("glue", quietly = TRUE, warn.conflicts = FALSE)
url <- glue::glue(
  "https://hryanjones.com/guess-my-word/board.html?",
  "difficulty=normal")
tbls <- read_html(url) 
html_nodes(tbls, "table") 
```

