---
title: 'Riddler: Sans Serifs'
author: ravic
date: '2021-06-04'
slug: riddler-sans-serifs
categories: []
tags:
  - riddler
---

[riddler-problem]: https://fivethirtyeight.com/features/can-you-decipher-the-secret-message/
[norvig-word-list]: https://norvig.com/ngrams/enable1.txt
[norvig-word-freq]: https://norvig.com/ngrams/count_1w100k.txt
[harking]: https://en.wikipedia.org/wiki/HARKing

From the Riddler, a problem concerning typography and topology. Eureka!

<!--more-->
<script src="/rmarkdown-libs/twitter-widget/widgets.js"></script>




### Problem

In [this week's Riddler][riddler-problem], via Alexander Zhang, we're invited to consider this *sans-serif* font example:  

![Font face](https://fivethirtyeight.com/wp-content/uploads/2021/06/Screen-Shot-2021-05-31-at-6.53.29-PM.png?w=700)

> Alexander thinks many of these letters are equivalent, but he leaves it to you to figure out how and why. He also has a message for you: **YIRTHA**. It may not look like much, but Alexander assures me that it is equivalent to exactly one word in the English language.

```{python read-words, include=FALSE}
import pandas as pd

# Get word list
pdf = (
    pd.read_csv("https://norvig.com/ngrams/enable1.txt", header=None, names=["word"])
    .assign(word = lambda x: x.word.str.upper())
)

# Get word frequencies for later
word_freq = (
    pd.read_csv(
        "https://norvig.com/ngrams/count_1w100k.txt", header=None, 
        names=["word", "freq"], sep="\t", 
        dtype={
            "word": str,
            "freq": "Int64"
        }
    )
    .assign(word = lambda x: x.word.str.strip())
)
```


```{python encoding, include = FALSE}
# A mapping of letterforms:
# 1. The first digit is the number of required strokes
# 2. The second character indicates the number of enclosed regions ('/' for zero,
#    '-' for one, and '=' for two enclosed regions, respectively).
translation_table = str.maketrans({
    "A": "2-", "B": "1=", "C": "1/", "D": "1-", "E": "2/",
    "F": "2/", "G": "1/", "H": "3/", "I": "1/", "J": "1/",
    "K": "3/", "L": "1/", "M": "1/", "N": "1/", "O": "1-",
    "P": "1-", "Q": "1-", "R": "2-", "S": "1/", "T": "2/",
    "U": "1/", "V": "1/", "W": "1/", "X": "3/", "Y": "2/",
    "Z": "1/",
})

# Determine candidates based on translation
candidates = (
    pdf
    .assign(encoded = lambda x: x.word.str.translate(translation_table))
    .loc[lambda x: x.encoded == "YIRTHA".translate(translation_table)]
)
candidates
```


### Answer

**`EUREKA`**!!


### Approach

*Sans-serif* is a hint. Let's look at the letterforms. First, let's find the  **number of distinct strokes**. If we have to pick up the pen to avoid traversing a section twice, that's a stroke. (`J` has one, `A` has two, and `H` has three.)  Then, we load [Peter Norvig's word list][norvig-word-list], map  letters to strokes, and find the answer.

Not so fast. Turns out, we get three possible candidates: `EUREKA`, `TWEAKY`, and `APATHY`. What next? Let's add a feature for the **number of enclosed regions** formed by the letter. (`J` has none, `A` has one, and `B` has two.) That gets us to a unique answer, **`EUREKA`**. 

Is this enough to decode Laurent Lessard's clue here? Let's run it this through our decoder and find out.


```{r clue, echo = FALSE, message = FALSE, warning = FALSE}
require("tweetrmd", quietly = TRUE, warn.conflicts = FALSE)
tweet_embed("https://twitter.com/LaurentLessard/status/1401256340555812870",
            plain = FALSE)
```


Seems like we've got it right:  

```{python decode-clue, echo=FALSE}
# Laurent Lessard's clue
clue = pd.DataFrame({"token": [
      "OYEORNRYMDU", "FOPDVOUZVAL", "HDWTOUDRQKVWJ", "USYXOCE", "GOZWAQMJZM", 
      "PADFAUOGGM", "QDRTZDUM"]})
clue = (
  clue
  .assign(idx = lambda x: x.index)
  .assign(encoded = lambda x: x.token.str.translate(translation_table))
)

decoded = (
    pdf
    .assign(encoded = lambda x: x.word.str.translate(translation_table))
    .merge(clue, how="inner", on="encoded")
    .merge(word_freq, how="left", on="word")
    .assign(freq = lambda x: x.freq.fillna(0))
    .groupby("token")
    .apply(lambda group: group.nlargest(1, columns="freq"))
    .reset_index(level = -1, drop=True)
    .sort_values(by="idx", ascending=True)
    .loc[:, ["word"]]
    ["word"].tolist()
    
)
decoded_sentence = ' '.join(decoded)
decoded_sentence
```







### Ambiguous words

Do all words have a unique encoding? If not, which words are the most ambiguous? The most ambiguous encoding captures the words below, including "minimum", "missing", "winning". Maybe unsurprisingly, they're composed of single-stroke, non-enclosing letterforms. 

```{python ambiguous-words, echo = FALSE}
pd.set_option('max_rows', None) 
pd.set_option('display.max_columns', None)

ambiguous_encodings = (
    pdf
    .assign(encoded = lambda x: x.word.str.translate(translation_table))
    .groupby("encoded")
    .agg(word_count=("word", "count"))
    .reset_index()
    .loc[lambda x: x.word_count > 1]
    .assign(encoding_length = lambda x: x.encoded.str.len())
#    .groupby("encoding_length")
#    .apply(lambda group: group.nlargest(1, columns="word_count"))
#    .reset_index(level = -1, drop=True)
    .loc[lambda x: x.encoding_length > 10]    
    .nlargest(1, columns="word_count")
)

ambiguous_words = (
    pdf
    .assign(encoded = lambda x: x.word.str.translate(translation_table))
    .merge(ambiguous_encodings, how="inner", on="encoded")
    .merge(word_freq, how="left", on="word")
    .sort_values(by=["word_count", "freq"], ascending=[False, False])
    .assign(word_length = lambda x: x.word.str.len())
    .loc[lambda x: x.freq > 0]
    .loc[lambda x: x.word_length > 5]
    .loc[:, ["word", "word_length", "word_count", "freq"]]
    .loc[:, ["word"]]
    ["word"].tolist()
)
ambiguous_words
```

One nice perk? This mapping doesn't care about British vs. American English -- the `S` and `Z` are equivalent.

***
### Update


```{r related, echo = FALSE, message = FALSE, warning = FALSE}
library("tweetrmd")
tweet_embed("https://twitter.com/1366811/status/1402364306725949441")
```




***

### Code 


```{python read-words, eval=FALSE, echo = TRUE}
```
```{python encoding, eval=FALSE, echo = TRUE}
```

***

*Additional code for this post on [Github](https://github.com/ravichandrasekaran/ravic_/blob/master/content/post/riddler/2021-06-04-riddler-sans-serifs.Rmd)*


<!--Appendix-->

```{r tweet-final, include=FALSE, eval=FALSE}
# Eureka! Found the answer to #ThisWeeksRiddler, @xaqwg.  https://ravic.netlify.app/post/riddler/riddler-sans-serifs/
```

```{python anchor-points, include=FALSE, eval=FALSE}

# An unused basic mapping of strokes
translation_table = str.maketrans({
    "A": "2", "B": "1", "C": "1", "D": "1", "E": "2",
    "F": "2", "G": "1", "H": "3", "I": "1", "J": "1",
    "K": "3", "L": "1", "M": "1", "N": "1", "O": "1",
    "P": "1", "Q": "1", "R": "2", "S": "1", "T": "2",
    "U": "1", "V": "1", "W": "1", "X": "2", "Y": "2",
    "Z": "1",
})


# An unused mapping using the number of anchor points
translation_table = str.maketrans({
    "A": "4", "B": "3", "C": "2", "D": "2", "E": "6",
    "F": "5", "G": "4", "H": "5", "I": "2", "J": "2",
    "K": "6", "L": "3", "M": "5", "N": "4", "O": "0",
    "P": "3", "Q": "1", "R": "5", "S": "2", "T": "4",
    "U": "2", "V": "3", "W": "5", "X": "5", "Y": "4",
    "Z": "4",
})
```

<!--img src="/riddler/2021-06-04-riddler-sans-serifs_files/eureka-6884380.png" alt="Light bulb with Eureka text"-->



```{r letter-mapping, echo = FALSE, eval = FALSE}
options(tidyverse.quiet = TRUE)
require("tidyverse", quietly = TRUE, warn.conflicts = FALSE)

tbl <- tibble(letter = LETTERS,
              stroke = c(2, 1, 1, 1, 2, 
                          2, 1, 3, 1, 1, 
                          3, 1, 1, 1, 1, 
                          1, 1, 2, 1, 2, 
                          1, 1, 1, 3, 2, 
                          1),
              enclosure = c(1, 2, 0, 1, 0, 
                             0, 0, 0, 0, 0,
                             0, 0, 0, 0, 1,
                             1, 1, 1, 0, 0, 
                             0, 0, 0, 0, 0,
                             0))
              
tbl %>% 
  nest(data = c(letter)) %>% 
  mutate(letters = purrr::map(data, "letter") %>% 
           purrr::map_chr(str_c, collapse = ",")) %>%
  select(stroke, enclosure, letters) %>%
  arrange(stroke, enclosure)
```



